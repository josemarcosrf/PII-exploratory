{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7feb8bb-c6d9-46b8-b010-121e514edffa",
   "metadata": {},
   "source": [
    "## üîó Langchain ReAct agent (Ollama)\n",
    "\n",
    "\n",
    "> **üößüößüößüößüöß This is still work in progress. Not clear how Agents could be best than just LLM directly üößüößüößüößüöß**\n",
    "\n",
    "This notebooks explores a ReAct Agent to identify `Personal Identifyable Information` (PII)\n",
    "\n",
    "> ‚ÑπÔ∏è More information at: [langchain / langraph ReAct Agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)\n",
    "\n",
    "\n",
    "This notebooks makes use of an Ollama running server. You'll need to first run:\n",
    "\n",
    "```bash\n",
    "ollama serve &\n",
    "ollama pull <some-model-you-want>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ff3168-1ed4-4f89-83e6-686826d8ab7e",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05d38d9c-9de1-4a5b-9cf4-6f9d06108703",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T20:38:43.310622Z",
     "iopub.status.busy": "2025-03-22T20:38:43.310176Z",
     "iopub.status.idle": "2025-03-22T20:39:26.525337Z",
     "shell.execute_reply": "2025-03-22T20:39:26.524525Z",
     "shell.execute_reply.started": "2025-03-22T20:38:43.310600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Cleaning up old version at /usr/local/lib/ollama\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "######################################################################## 100.0%#############################                                      50.9%\n",
      ">>> Adding ollama user to render group...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "\u001b[2mUsing Python 3.11.7 environment at: /usr\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m5 packages\u001b[0m \u001b[2min 7ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install uv\n",
    "# !curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Install Ollama\n",
    "!sudo apt install lshw # for hardware detection\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Install langchain deps\n",
    "!uv pip install --system \\\n",
    "    langgraph langchain langchain-core langchain-community langchain-ollama duckduckgo-search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f904ea4-daae-4721-93fa-a6ae6194cb8d",
   "metadata": {},
   "source": [
    "## ü¶ú LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10885abe-1905-4415-9525-28fb41d35355",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T20:58:21.475492Z",
     "iopub.status.busy": "2025-03-22T20:58:21.475227Z",
     "iopub.status.idle": "2025-03-22T20:58:24.497210Z",
     "shell.execute_reply": "2025-03-22T20:58:24.496514Z",
     "shell.execute_reply.started": "2025-03-22T20:58:21.475471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve for x, we need to isolate the variable x on one side of the equation.\n",
      "\n",
      "First, add 9 to both sides of the equation:\n",
      "\n",
      "43x - 9 + 9 = 120 + 9\n",
      "43x = 129\n",
      "\n",
      "Next, divide both sides by 43:\n",
      "\n",
      "43x / 43 = 129 / 43\n",
      "x = 3\n",
      "\n",
      "So, the value of x is 3.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# ü§Ø Classic langchain... 4 different types of importing an Ollama LLM, but...\n",
    "# üëâ from langchain_community.chat_models import ChatOllama\n",
    "# üëâ from langchain_ollama import OllamaLLM\n",
    "# üëâ from langchain_community.llms import Ollama\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "\n",
    "# ===================================== üëá Configure as needed =================================\n",
    "sys.tracebacklimit = -2\n",
    "os.environ[\"TRACEBACK_LIMIT\"] = \"2\"\n",
    "\n",
    "# Or any other model capable os using tools\n",
    "MODEL_ID = \"llama3.2\" # \"jacob-ebey/phi4-tools\"\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "# ===================================== üëÜ Configure as needed =================================\n",
    "\n",
    "\n",
    "# Initialize Ollama\n",
    "# llm = OllamaLLM(model=MODEL_ID, base_url=OLLAMA_BASE_URL, temperature=0.0)\n",
    "json_llm = ChatOllama(model=MODEL_ID, base_url=OLLAMA_BASE_URL, temperature=0.0, format=\"json\")\n",
    "llm = ChatOllama(model=MODEL_ID, base_url=OLLAMA_BASE_URL, temperature=0.0)\n",
    "\n",
    "\n",
    "prompt = \"Please solve: 43x - 9 = 120\"\n",
    "messages = [\n",
    "    (\"system\", \"You are a helpful math assistant.\"),\n",
    "    (\"human\", prompt),\n",
    "]\n",
    "\n",
    "res = llm.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57e67e9a-b10a-4d1c-9994-7f36d5692ad8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T20:35:19.453177Z",
     "iopub.status.busy": "2025-03-22T20:35:19.452936Z",
     "iopub.status.idle": "2025-03-22T20:35:21.036175Z",
     "shell.execute_reply": "2025-03-22T20:35:21.035561Z",
     "shell.execute_reply.started": "2025-03-22T20:35:19.453158Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "del text_llm\n",
    "del json_llm\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081ac3d9-6b29-4fba-a0a5-6fd54d44bd8a",
   "metadata": {},
   "source": [
    "## ü§ñ Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b20cde58-9a99-4b31-b315-fcc3b619b246",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T22:12:22.607610Z",
     "iopub.status.busy": "2025-03-22T22:12:22.607329Z",
     "iopub.status.idle": "2025-03-22T22:12:22.627000Z",
     "shell.execute_reply": "2025-03-22T22:12:22.626315Z",
     "shell.execute_reply.started": "2025-03-22T22:12:22.607586Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "# ===================================== üëá Configure as needed =================================\n",
    "TEMPLATE = \"\"\"\n",
    "In this chunk of markdown text:\n",
    "\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "{request}\n",
    "\n",
    "Answer with a bullet-point list if any are found. Otherwise respond 'None'\n",
    "\"\"\"\n",
    "# ===================================== üëÜ Configure as needed =================================\n",
    "\n",
    "\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "# Define a simple tool\n",
    "@tool\n",
    "def dummy_tool(query: str) -> str:\n",
    "    \"\"\"Search for information about a topic.\"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "tools = [search_tool]\n",
    "tool_names = [t.name for t in tools]\n",
    "\n",
    "\n",
    "# Create the ReAct agent\n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    # prompt=PROMPT_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ade9c1d-e18d-453d-bdba-12557c1dfb80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T22:12:25.642574Z",
     "iopub.status.busy": "2025-03-22T22:12:25.641836Z",
     "iopub.status.idle": "2025-03-22T22:12:30.799789Z",
     "shell.execute_reply": "2025-03-22T22:12:30.799077Z",
     "shell.execute_reply.started": "2025-03-22T22:12:25.642551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "The final answer to the user's question \"What is an agent in the context of LLMs?\" is:\n",
      "\n",
      "In the context of Large Language Models (LLMs), an agent refers to a software component that can perceive its environment, learn from it, reason about it, and take actions to achieve a specific goal. Agents in LLMs can be categorized into different types, including:\n",
      "\n",
      "1. Router Agents: These agents work together with other agents to achieve a common goal.\n",
      "2. Planning Agents: These agents use their planning capabilities to select which actions to take.\n",
      "3. Multimodal Agents: These agents process and generate content across various formats, including audio, images, and video.\n",
      "\n",
      "Agents in LLMs can be used to build intelligent systems that can coordinate and solve complex tasks collectively at scale. They have a wide range of applications, such as project management bots, collaborative decision-making tools, and multimodal interfaces.\n"
     ]
    }
   ],
   "source": [
    "messages = [(\"human\", \"What is an agent in the context of LLMs?\")]\n",
    "\n",
    "# Run the agent\n",
    "response = agent.invoke({\"messages\": messages})\n",
    "print(\"Response:\")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
