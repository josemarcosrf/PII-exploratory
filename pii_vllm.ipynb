{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "056b6503",
   "metadata": {},
   "source": [
    "# vLLM direct redaction\n",
    "\n",
    "> **⚠️⚠️⚠️ vLLM complicates running larger models compared to Ollama, which makes performance worse. ⚠️⚠️⚠️**\n",
    "\n",
    "This notebooks explores using LLM models directly to identify `Personal Identifyable Information` (PII).\n",
    "\n",
    "> ℹ️ We use [vLLM](https://docs.vllm.ai/en/stable/) and markdown inputs as produced by something like [docLing](https://docs.vllm.ai/en/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91650e0b",
   "metadata": {},
   "source": [
    "## ⚙️ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "798bf520",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T18:10:26.094117Z",
     "iopub.status.busy": "2025-03-26T18:10:26.093392Z",
     "iopub.status.idle": "2025-03-26T18:11:11.495129Z",
     "shell.execute_reply": "2025-03-26T18:11:11.493732Z",
     "shell.execute_reply.started": "2025-03-26T18:10:26.094089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading uv 0.6.10 x86_64-unknown-linux-gnu\n",
      "no checksums to verify\n",
      "installing to /root/.local/bin\n",
      "  uv\n",
      "  uvx\n",
      "everything's installed!\n",
      "\u001b[2mUsing Python 3.11.7 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m126 packages\u001b[0m \u001b[2min 833ms\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m83 packages\u001b[0m \u001b[2min 41.13s\u001b[0m\u001b[0m                                           \n",
      "\u001b[2mUninstalled \u001b[1m17 packages\u001b[0m \u001b[2min 1.08s\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m83 packages\u001b[0m \u001b[2min 431ms\u001b[0m\u001b[0m                              \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mairportsdata\u001b[0m\u001b[2m==20250224\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mastor\u001b[0m\u001b[2m==0.8.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mblake3\u001b[0m\u001b[2m==1.0.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcompressed-tensors\u001b[0m\u001b[2m==0.9.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdepyf\u001b[0m\u001b[2m==0.18.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdiskcache\u001b[0m\u001b[2m==5.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdistro\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdnspython\u001b[0m\u001b[2m==2.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1meinops\u001b[0m\u001b[2m==0.8.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1memail-validator\u001b[0m\u001b[2m==2.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastapi\u001b[0m\u001b[2m==0.115.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastapi-cli\u001b[0m\u001b[2m==0.0.7\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.13.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.18.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgguf\u001b[0m\u001b[2m==0.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mh11\u001b[0m\u001b[2m==0.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpcore\u001b[0m\u001b[2m==1.0.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttptools\u001b[0m\u001b[2m==0.6.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.28.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.20.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.29.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1minteregular\u001b[0m\u001b[2m==0.3.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjiter\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.2.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mllguidance\u001b[0m\u001b[2m==0.7.10\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.43.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlm-format-enforcer\u001b[0m\u001b[2m==0.10.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkdown-it-py\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmdurl\u001b[0m\u001b[2m==0.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmistral-common\u001b[0m\u001b[2m==1.5.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmsgpack\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmsgspec\u001b[0m\u001b[2m==0.19.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.60.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.4.5.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.1.0.70\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.5.147\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.1.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.3.1.170\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.6.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.21.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==1.68.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopencv-python-headless\u001b[0m\u001b[2m==4.11.0.86\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1moutlines\u001b[0m\u001b[2m==0.1.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1moutlines-core\u001b[0m\u001b[2m==0.1.26\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpartial-json-parser\u001b[0m\u001b[2m==0.2.1.1.post5\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==9.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.1.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mprometheus-client\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprometheus-client\u001b[0m\u001b[2m==0.21.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprometheus-fastapi-instrumentator\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpycountry\u001b[0m\u001b[2m==24.6.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==1.10.14\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.10.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.27.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-multipart\u001b[0m\u001b[2m==0.0.20\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mray\u001b[0m\u001b[2m==2.44.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==13.9.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich-toolkit\u001b[0m\u001b[2m==0.13.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.5.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1msentencepiece\u001b[0m\u001b[2m==0.1.99\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msentencepiece\u001b[0m\u001b[2m==0.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mstarlette\u001b[0m\u001b[2m==0.46.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.13.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtiktoken\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.15.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.1.1+cu121\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.6.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorchaudio\u001b[0m\u001b[2m==2.1.1+cu121\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchaudio\u001b[0m\u001b[2m==2.6.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.16.1+cu121\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.21.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.35.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.50.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==2.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.2.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtyper\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyper\u001b[0m\u001b[2m==0.15.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.13.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1muvicorn\u001b[0m\u001b[2m==0.34.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1muvloop\u001b[0m\u001b[2m==0.21.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mvllm\u001b[0m\u001b[2m==0.8.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwatchfiles\u001b[0m\u001b[2m==1.0.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwebsockets\u001b[0m\u001b[2m==15.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxformers\u001b[0m\u001b[2m==0.0.29.post2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxgrammar\u001b[0m\u001b[2m==0.1.16\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install uv\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Install vLLM\n",
    "!uv pip install --system vllm python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb54178",
   "metadata": {},
   "source": [
    "## 🦜 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a11e7724",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T18:13:15.676838Z",
     "iopub.status.busy": "2025-03-26T18:13:15.676493Z",
     "iopub.status.idle": "2025-03-26T18:15:14.063773Z",
     "shell.execute_reply": "2025-03-26T18:15:14.062557Z",
     "shell.execute_reply.started": "2025-03-26T18:13:15.676813Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4d79c2a984447798b1e570b22edd52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946a8e56b04a4002a52e9ff15207eea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/10.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-4-mini-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-26 18:13:15 [config.py:208] Replacing legacy 'type' key with 'rope_type'\n",
      "WARNING 03-26 18:13:16 [config.py:2614] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-26 18:13:29 [config.py:585] This model supports multiple tasks: {'generate', 'score', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 03-26 18:13:29 [arg_utils.py:1854] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "INFO 03-26 18:13:29 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.2) with config: model='microsoft/Phi-4-mini-instruct', speculative_config=None, tokenizer='microsoft/Phi-4-mini-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=10000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=microsoft/Phi-4-mini-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0a717e35414f56bd53f3826e166deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.93k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513b5b2e8ccd414d95c699242b9a9954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/3.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8790e98fa1c849f2a2f6de4188604a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3768a694014571b39d44a2fa5d7918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/15.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c57925e66b4a369fcf2d91745f07e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/249 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc96622ad8743179c082e4b7f8d16d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/587 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5d589684014b49952baff0b4536cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-26 18:13:34 [cuda.py:239] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-26 18:13:34 [cuda.py:288] Using XFormers backend.\n",
      "INFO 03-26 18:13:35 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 03-26 18:13:35 [model_runner.py:1110] Starting to load model microsoft/Phi-4-mini-instruct...\n",
      "INFO 03-26 18:13:35 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246282fbb4bf48ba991356491f633495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.77G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b69f8b19d9644b999ad259f66ebdf2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-26 18:13:54 [weight_utils.py:281] Time spent downloading weights for microsoft/Phi-4-mini-instruct: 19.354084 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be121fec228d4185abc8bda7af1cd9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1346b240862f4fd3ae0fbcc129a94854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-26 18:14:00 [loader.py:447] Loading weights took 5.39 seconds\n",
      "INFO 03-26 18:14:00 [model_runner.py:1146] Model loading took 7.1694 GB and 25.053579 seconds\n",
      "INFO 03-26 18:14:03 [worker.py:267] Memory profiling takes 2.12 seconds\n",
      "INFO 03-26 18:14:03 [worker.py:267] the current vLLM instance can use total_gpu_memory (15.73GiB) x gpu_memory_utilization (0.90) = 14.16GiB\n",
      "INFO 03-26 18:14:03 [worker.py:267] model weights take 7.17GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.88GiB; the rest of the memory reserved for KV Cache is 5.06GiB.\n",
      "INFO 03-26 18:14:03 [executor_base.py:111] # cuda blocks: 2591, # CPU blocks: 2048\n",
      "INFO 03-26 18:14:03 [executor_base.py:116] Maximum concurrency for 10000 tokens per request: 4.15x\n",
      "INFO 03-26 18:14:07 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [01:06<00:00,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-26 18:15:13 [model_runner.py:1570] Graph capturing finished in 67 secs, took 0.39 GiB\n",
      "INFO 03-26 18:15:14 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 73.22 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ===================================== 👇 Configure as needed =================================\n",
    "assert os.environ[\"HF_TOKEN\"], \"Plase set the HF_TOKEN = hf...\" # HuggingFace API token\"\n",
    "\n",
    "# Or: 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "# Or: 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "MODEL_ID = \"microsoft/Phi-4-mini-instruct\"\n",
    "\n",
    "MAX_MODEL_LEN = 10000    # Adjust depending on avaialble memory\n",
    "DTYPE = \"float16\"        # Change to bfloat16 if GPU is capable (cuda > 8.0)\n",
    "# ===================================== 👆 Configure as needed =================================\n",
    "\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL_ID,\n",
    "    max_model_len=MAX_MODEL_LEN,  \n",
    "    trust_remote_code=True, \n",
    "    dtype=DTYPE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d24e9048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T18:23:17.955590Z",
     "iopub.status.busy": "2025-03-22T18:23:17.954827Z",
     "iopub.status.idle": "2025-03-22T18:23:18.725848Z",
     "shell.execute_reply": "2025-03-22T18:23:18.724926Z",
     "shell.execute_reply.started": "2025-03-22T18:23:17.955563Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913c51b4",
   "metadata": {},
   "source": [
    "### 🗣️ Check chatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "962a3b99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T18:15:14.069044Z",
     "iopub.status.busy": "2025-03-26T18:15:14.068771Z",
     "iopub.status.idle": "2025-03-26T18:15:18.856397Z",
     "shell.execute_reply": "2025-03-26T18:15:18.855532Z",
     "shell.execute_reply.started": "2025-03-26T18:15:14.069019Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate(prompt: str, sampling_params = {}, pbar=False, **chat_args):\n",
    "    # Compose the messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    # Compose the sampling parameters\n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=1000,\n",
    "        temperature=0.0,\n",
    "        **sampling_params,\n",
    "    )\n",
    "    # Inference\n",
    "    output = llm.chat(\n",
    "        messages=messages, sampling_params=sampling_params, use_tqdm=pbar, **chat_args\n",
    "    )\n",
    "    return output[0].outputs[0].text\n",
    "\n",
    "\n",
    "# Run inference\n",
    "ans = generate(\"hey there. What can you do?\", pbar=True)\n",
    "print(f\"🗣️ Answer: {ans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8bea01",
   "metadata": {},
   "source": [
    "## 📚 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a0e37f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T18:15:39.554890Z",
     "iopub.status.busy": "2025-03-26T18:15:39.554243Z",
     "iopub.status.idle": "2025-03-26T18:15:41.907932Z",
     "shell.execute_reply": "2025-03-26T18:15:41.906665Z",
     "shell.execute_reply.started": "2025-03-26T18:15:39.554859Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from utils import split_markdown_by_spans\n",
    "\n",
    "\n",
    "DATA_DIR = Path(\"/datasets/client-data-us/\")\n",
    "\n",
    "md_docs = list(DATA_DIR.rglob(\"**/*.md\"))\n",
    "print(f\"Total markdown documents: {len(md_docs)}\")\n",
    "\n",
    "# Read the first document\n",
    "text = md_docs[0].open(\"r\").read()\n",
    "print(text[:200])\n",
    "\n",
    "# Split into chunks\n",
    "spans = split_markdown_by_spans(text)\n",
    "for span_id, text in spans.items():\n",
    "    print(f\"ID: {span_id}\")\n",
    "    print(f\"Text: {text[:50]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7524a2f",
   "metadata": {},
   "source": [
    "## 🫥 Anonymisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94fd4b7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T19:32:54.136369Z",
     "iopub.status.busy": "2025-03-26T19:32:54.136035Z",
     "iopub.status.idle": "2025-03-26T19:33:12.194120Z",
     "shell.execute_reply": "2025-03-26T19:33:12.193187Z",
     "shell.execute_reply.started": "2025-03-26T19:32:54.136344Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm.rich import tqdm\n",
    "from pprint import pformat\n",
    "\n",
    "TEMPLATE = \"\"\"\n",
    "In this chunk of markdown text:\n",
    "\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "{request}\n",
    "\n",
    "Answer with a bullet-point list if any are found. Otherwise respond 'None'\n",
    "\"\"\"\n",
    "\n",
    "requests = {\n",
    "    \"orgs\": (\n",
    "        \"What are the companies mentioned in the text? \"\n",
    "        \"(Do not include placeholders like: 'Developer', 'Customer', 'Distributor' or similar.)\",\n",
    "    ),\n",
    "    \"loc\": \"What locations or addresses mentioned in the text?\",\n",
    "    \"contact\": \"What telephone-numbers or emails are mentioned in the text?\",\n",
    "    \"people\": \"What person's are mentioned in the text?\",\n",
    "    \"date\": \"What dates are mentioned in the text?\"\n",
    "}\n",
    "\n",
    "entities = defaultdict(lambda: defaultdict(list))\n",
    "doc_iter = tqdm(md_docs)\n",
    "for md_doc in doc_iter:\n",
    "    doc_name = md_doc.stem\n",
    "    doc_iter.set_description(f\"📄 {doc_name}...\")\n",
    "    text = md_doc.open().read()\n",
    "    spans = split_markdown_by_spans(text)\n",
    "    for span_id, text in spans.items():\n",
    "        for k, question in requests.items():\n",
    "            ans = generate(TEMPLATE.format(context=text, request=question))\n",
    "            if not \"None\" in ans:\n",
    "                entities[doc_name][k] += [a.strip() for a in ans.split(\"- \") if a]\n",
    "                \n",
    "    break\n",
    "                \n",
    "\n",
    "print(pformat(entities))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cabb502",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T18:30:48.412342Z",
     "iopub.status.busy": "2025-03-26T18:30:48.411843Z",
     "iopub.status.idle": "2025-03-26T18:30:48.430376Z",
     "shell.execute_reply": "2025-03-26T18:30:48.428122Z",
     "shell.execute_reply.started": "2025-03-26T18:30:48.412299Z"
    }
   },
   "outputs": [],
   "source": [
    "# map each entity to a placeholder and substitute in the original text\n",
    "placeholders = {}\n",
    "for doc_name, doc_entities in entities.items():\n",
    "    placeholders[doc_name] = {}\n",
    "    for ent_type, ent_list in doc_entities.items():\n",
    "        for i, ent in enumerate(ent_list):\n",
    "            placeholders[doc_name][ent] = f\"{ent_type.upper()}_{i}\" \n",
    "\n",
    "print(pformat(placeholders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cb80b08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T18:35:25.761452Z",
     "iopub.status.busy": "2025-03-26T18:35:25.760160Z",
     "iopub.status.idle": "2025-03-26T18:35:25.922608Z",
     "shell.execute_reply": "2025-03-26T18:35:25.921726Z",
     "shell.execute_reply.started": "2025-03-26T18:35:25.761417Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import re\n",
    "\n",
    "\n",
    "def mask_text(text, entities):\n",
    "    masked_text = copy.copy(text)\n",
    "    for ent, placeholder in entities.items():\n",
    "        if ent in [\"Customer\", \"Developer\", \"Distributor\"]:\n",
    "            continue\n",
    "        print(f\"{ent} --> {placeholder}\")\n",
    "        masked_text = re.sub(ent, f\"[{placeholder}]\" , masked_text, count=0, flags=0)\n",
    "\n",
    "    return masked_text\n",
    "    \n",
    "    \n",
    "doc = md_docs[0]\n",
    "doc_name = doc.stem\n",
    "doc_text = doc.open().read()\n",
    "print(mask_text(doc_text, placeholders[doc_name]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
