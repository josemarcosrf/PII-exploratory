{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aa5fcae",
   "metadata": {},
   "source": [
    "# vLLM direct redaction\n",
    "\n",
    "> **⚠️⚠️⚠️ vLLM complicates running larger models compared to Ollama, which makes performance worse. ⚠️⚠️⚠️**\n",
    "\n",
    "This notebooks explores using LLM models directly to identify `Personal Identifyable Information` (PII).\n",
    "\n",
    "> ℹ️ We use [vLLM](https://docs.vllm.ai/en/stable/) and markdown inputs as produced by something like [docLing](https://docs.vllm.ai/en/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ded3d",
   "metadata": {},
   "source": [
    "## ⚙️ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642549d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install uv\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Install vLLM\n",
    "!uv pip install --system vllm python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345b0e4e",
   "metadata": {},
   "source": [
    "## 🦜 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2738f73f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ===================================== 👇 Configure as needed =================================\n",
    "assert os.environ[\"HF_TOKEN\"], \"Plase set the HF_TOKEN = hf...\" # HuggingFace API token\"\n",
    "\n",
    "# Or: 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "# Or: 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "MODEL_ID = \"microsoft/Phi-4-mini-instruct\"\n",
    "\n",
    "MAX_MODEL_LEN = 10000    # Adjust depending on avaialble memory\n",
    "DTYPE = \"float16\"        # Change to bfloat16 if GPU is capable (cuda > 8.0)\n",
    "# ===================================== 👆 Configure as needed =================================\n",
    "\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL_ID,\n",
    "    max_model_len=MAX_MODEL_LEN,  \n",
    "    trust_remote_code=True, \n",
    "    dtype=DTYPE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad5ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e200ab7",
   "metadata": {},
   "source": [
    "### 🗣️ Check chatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28319d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str, sampling_params = {}, pbar=False, **chat_args):\n",
    "    # Compose the messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    # Compose the sampling parameters\n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=1000,\n",
    "        temperature=0.0,\n",
    "        **sampling_params,\n",
    "    )\n",
    "    # Inference\n",
    "    output = llm.chat(\n",
    "        messages=messages, sampling_params=sampling_params, use_tqdm=pbar, **chat_args\n",
    "    )\n",
    "    return output[0].outputs[0].text\n",
    "\n",
    "\n",
    "# Run inference\n",
    "ans = generate(\"hey there. What can you do?\", pbar=True)\n",
    "print(f\"🗣️ Answer: {ans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4a0c88",
   "metadata": {},
   "source": [
    "## 📚 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83414481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from utils import split_markdown_by_spans\n",
    "\n",
    "\n",
    "DATA_DIR = Path(\"/datasets/client-data-us/\")\n",
    "\n",
    "md_docs = list(DATA_DIR.rglob(\"**/*.md\"))\n",
    "print(f\"Total markdown documents: {len(md_docs)}\")\n",
    "\n",
    "# Read the first document\n",
    "text = md_docs[0].open(\"r\").read()\n",
    "print(text[:200])\n",
    "\n",
    "# Split into chunks\n",
    "spans = split_markdown_by_spans(text)\n",
    "for span_id, text in spans.items():\n",
    "    print(f\"ID: {span_id}\")\n",
    "    print(f\"Text: {text[:50]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0f3521",
   "metadata": {},
   "source": [
    "## 🫥 Anonymisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b414ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm.rich import tqdm\n",
    "from pprint import pformat\n",
    "\n",
    "TEMPLATE = \"\"\"\n",
    "In this chunk of markdown text:\n",
    "\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "{request}\n",
    "\n",
    "Answer with a bullet-point list if any are found. Otherwise respond 'None'\n",
    "\"\"\"\n",
    "\n",
    "requests = {\n",
    "    \"orgs\": (\n",
    "        \"What are the companies mentioned in the text? \"\n",
    "        \"(Do not include placeholders like: 'Developer', 'Customer', 'Distributor' or similar.)\",\n",
    "    ),\n",
    "    \"loc\": \"What locations or addresses mentioned in the text?\",\n",
    "    \"contact\": \"What telephone-numbers or emails are mentioned in the text?\",\n",
    "    \"people\": \"What person's are mentioned in the text?\",\n",
    "    \"date\": \"What dates are mentioned in the text?\"\n",
    "}\n",
    "\n",
    "entities = defaultdict(lambda: defaultdict(list))\n",
    "doc_iter = tqdm(md_docs)\n",
    "for md_doc in doc_iter:\n",
    "    doc_name = md_doc.stem\n",
    "    doc_iter.set_description(f\"📄 {doc_name}...\")\n",
    "    text = md_doc.open().read()\n",
    "    spans = split_markdown_by_spans(text)\n",
    "    for span_id, text in spans.items():\n",
    "        for k, question in requests.items():\n",
    "            ans = generate(TEMPLATE.format(context=text, request=question))\n",
    "            if not \"None\" in ans:\n",
    "                entities[doc_name][k] += [a.strip() for a in ans.split(\"- \") if a]\n",
    "                \n",
    "    break\n",
    "                \n",
    "\n",
    "print(pformat(entities))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429efa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map each entity to a placeholder and substitute in the original text\n",
    "placeholders = {}\n",
    "for doc_name, doc_entities in entities.items():\n",
    "    placeholders[doc_name] = {}\n",
    "    for ent_type, ent_list in doc_entities.items():\n",
    "        for i, ent in enumerate(ent_list):\n",
    "            placeholders[doc_name][ent] = f\"{ent_type.upper()}_{i}\" \n",
    "\n",
    "print(pformat(placeholders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d4c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import re\n",
    "\n",
    "\n",
    "def mask_text(text, entities):\n",
    "    masked_text = copy.copy(text)\n",
    "    for ent, placeholder in entities.items():\n",
    "        if ent in [\"Customer\", \"Developer\", \"Distributor\"]:\n",
    "            continue\n",
    "        print(f\"{ent} --> {placeholder}\")\n",
    "        masked_text = re.sub(ent, f\"[{placeholder}]\" , masked_text, count=0, flags=0)\n",
    "\n",
    "    return masked_text\n",
    "    \n",
    "    \n",
    "doc = md_docs[0]\n",
    "doc_name = doc.stem\n",
    "doc_text = doc.open().read()\n",
    "print(mask_text(doc_text, placeholders[doc_name]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
