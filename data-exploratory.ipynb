{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c298565-0fa8-4b1f-898e-6116f873ba70",
   "metadata": {},
   "source": [
    "## âš™ï¸ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10800f49-74b3-4234-b5a7-388931fa5d77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading uv 0.6.12 x86_64-unknown-linux-gnu\n",
      "no checksums to verify\n",
      "installing to /root/.local/bin\n",
      "  uv\n",
      "  uvx\n",
      "everything's installed!\n",
      "\u001b[2mUsing Python 3.11.7 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m171 packages\u001b[0m \u001b[2min 121ms\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m6 packages\u001b[0m \u001b[2min 81ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdocling\u001b[0m\u001b[2m==2.28.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.30.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.11.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpymupdf\u001b[0m\u001b[2m==1.25.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.51.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mvllm\u001b[0m\u001b[2m==0.8.3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install uv\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Install deps\n",
    "!uv pip install docling pymupdf vllm transformers\n",
    "# !uv pip install docling pymupdf vllm transformers\n",
    "# !uv pip install 'pydantic==1.10.14' 'protobuf==3.20.3' 'accelerate>=0.26.0' \n",
    "# !uv pip install flash-attn #--no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbf41c6-97b6-40c2-b4e3-1fc60497c802",
   "metadata": {},
   "source": [
    "## ğŸ“š AAA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7161c5ca-3fcb-4f67-b08c-752a5f16a248",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Found: 012100046665\n",
      "ğŸ“ Found: 012200023252\n",
      "ğŸ“ Found: 012300021267\n",
      "ğŸ“ Found: 012300047931\n",
      "ğŸ“ Found: 012300051365\n",
      "ğŸ“ Found: 022200022285\n",
      "ğŸ“š Total items: 66\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from utils import split_markdown_by_spans\n",
    "\n",
    "DATA_DIR = Path(\"/datasets/client-data-us/AAA/Redaction\")\n",
    "\n",
    "# Map between unredacted and redacted files\n",
    "u2r = {}\n",
    "for doc_dir in DATA_DIR.glob(\"*/\"):\n",
    "    print(f\"ğŸ“ Found: {doc_dir.name}\")\n",
    "    redacted_files = [f for f in (doc_dir / \"Redacted\").rglob(\"*.*\") if f.suffix != \".md\"]\n",
    "    unredacted_files = [f for f in (doc_dir / \"Unredacted\").rglob(\"*.*\") if f.suffix != \".md\"]\n",
    "    redacted_fnames = [rf.name for rf in redacted_files]\n",
    "    for i, uf in enumerate(unredacted_files):\n",
    "        rfn = uf.name.replace(uf.suffix, \"-redacted\" + uf.suffix)\n",
    "        if rfn in redacted_fnames:\n",
    "            u2r[uf] = redacted_files[i]\n",
    "        \n",
    "r2u = {v:k for k,v in u2r.items()} # redacted to unredacted\n",
    "print(f\"ğŸ“š Total items: {len(r2u)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aacaacad-ccc9-42dc-8dda-b26a68669345",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['012100046665', '012200023252', '012300021267', '012300047931', '012300051365', '022200022285'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# group data by case\n",
    "case_files = defaultdict(list)\n",
    "for fpath in u2r:\n",
    "    case_files[fpath.parent.parent.parent.name].append(fpath)\n",
    "    \n",
    "case_files.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85eec977-b307-4508-a3a9-b2228fa1d063",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Case '012100046665': documents: 34 | pages: 91\n",
      "ğŸ“‚ Case '012200023252': documents: 2 | pages: 5\n",
      "ğŸ“‚ Case '012300021267': documents: 8 | pages: 269\n",
      "ğŸ“‚ Case '012300047931': documents: 16 | pages: 402\n",
      "ğŸ“‚ Case '012300051365': documents: 3 | pages: 17\n",
      "ğŸ“‚ Case '022200022285': documents: 3 | pages: 15\n",
      "ğŸª£ Total pages: 799\n",
      "\n",
      "ğŸ“ Average pages per doc: 12.106060606060606\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "\n",
    "\n",
    "def pdf_page_count(pdf_path: Path | str):\n",
    "    return fitz.open(pdf_path).page_count\n",
    "\n",
    "\n",
    "# Get a count of pages for all the documents\n",
    "page_counts = {}\n",
    "for fp in u2r: \n",
    "    if fp.suffix == \".pdf\":\n",
    "        page_counts[fp] = pdf_page_count(fp)\n",
    "        \n",
    "# Document and Page count per case\n",
    "for case, files in case_files.items():\n",
    "    total_pages = sum(page_counts[f] for f in files if f in page_counts)\n",
    "    print(f\"ğŸ“‚ Case '{case}': documents: {len(files)} | pages: {total_pages}\")\n",
    "\n",
    "# Print everything\n",
    "page_avg_count = sum(c for c in page_counts.values()) / len(page_counts)\n",
    "print(f\"ğŸª£ Total pages: {sum(page_counts.values())}\")\n",
    "print(f\"\\nğŸ“ Average pages per doc: {page_avg_count:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531b678d-fcbf-4c8d-bfc7-1546532c02c8",
   "metadata": {},
   "source": [
    "## ğŸ“ Convert to Markdown\n",
    "\n",
    "Here we use `docling` to produce markdown output files. \n",
    "\n",
    "> ğŸ’¡ We can either use the CLI or do it from python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc16b39-b92a-4376-881a-2c9d4d1534e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.pipeline.vlm_pipeline import VlmPipeline\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    AcceleratorDevice,\n",
    "    VlmPipelineOptions,\n",
    "    granite_vision_vlm_conversion_options,\n",
    "    smoldocling_vlm_conversion_options,\n",
    "    smoldocling_vlm_mlx_conversion_options,\n",
    ")\n",
    "\n",
    "\n",
    "class ExportFormat(str, Enum):\n",
    "    Markdown:str = \"md\"\n",
    "    HTML:str = \"html\"\n",
    "\n",
    "    \n",
    "def build_vllm_pipeline_options():\n",
    "    import platform\n",
    "    import torch\n",
    "    \n",
    "    pipeline_options = VlmPipelineOptions()\n",
    "\n",
    "    # On GPU systems, enable flash_attention_2 with CUDA\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"ğŸš€ Using CUDA for GPU acceleration.\")\n",
    "        pipeline_options.accelerator_options.device = AcceleratorDevice.CUDA\n",
    "        # pipeline_options.accelerator_options.cuda_use_flash_attention2 = True\n",
    "\n",
    "    ## Pick a VLM model:\n",
    "    if \"arm\" in platform.processor():\n",
    "        # Fast Apple Silicon friendly implementation for SmolDocling-256M via MLX\n",
    "        print(\"ğŸ Using Apple Silicon implementation for SmolDocling-256M via MLX.\")\n",
    "        pipeline_options.vlm_options = smoldocling_vlm_mlx_conversion_options\n",
    "    else:\n",
    "        # Otherwise, we choose SmolDocling-256M by default\n",
    "        print(\"ğŸ–¥ï¸ Using default SmolDocling-256M model for VLM conversion.\")\n",
    "        pipeline_options.vlm_options = smoldocling_vlm_conversion_options\n",
    "\n",
    "    return pipeline_options\n",
    "\n",
    "\n",
    "def docling_convert(\n",
    "    converter,\n",
    "    file_path: Path | str, \n",
    "    export_format: ExportFormat = ExportFormat.Markdown, \n",
    ") -> str:\n",
    "    \"\"\"Convert a PDF file to text using Docling default conversion.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the PDF file\n",
    "        export_format (ExportFormat): Export format (Markdown or HTML)\n",
    "    \"\"\"\n",
    "\n",
    "    # Use the default Docling conversion service\n",
    "    file_path = Path(file_path)\n",
    "    print(f\"ğŸª„ {file_path.name} â¡ï¸ {export_format.name}\")\n",
    "    result = converter.convert(file_path)\n",
    "    if export_format == ExportFormat.Markdown:\n",
    "        return result.document.export_to_markdown()\n",
    "    elif export_format == ExportFormat.HTML:\n",
    "        return result.document.export_to_html()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported format: {export_format}\")\n",
    "        \n",
    "        \n",
    "def convert_doc(doc: Path | str):\n",
    "    for redacted_file in r2u:\n",
    "        doc = Path(doc)\n",
    "        outfile = rf.with_suffix(\".md\")\n",
    "        if outfile.exists():\n",
    "            print(f\"ğŸ¦˜ Skipping {rf.name}\")\n",
    "            continue\n",
    "\n",
    "        md_text = docling_convert(converter, doc)\n",
    "        with outfile.open(\"w\") as f:\n",
    "            f.write(md_text)\n",
    "\n",
    "            \n",
    "print(\"ğŸ£ Initializing converter...\")\n",
    "converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_cls=VlmPipeline,\n",
    "            pipeline_options=build_vllm_pipeline_options(),\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Convert redacted files to makrdown\n",
    "for redacted_file in r2u:\n",
    "    convert_doc(Path(redacted_file))\n",
    "\n",
    "# Convert unredacted files to makrdown\n",
    "for unredacted_file in u2r:\n",
    "    convert_doc(Path(unredacted_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a380ae-fa55-4292-b026-453b0e55151e",
   "metadata": {},
   "source": [
    "## ğŸª£ Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c46a9dee-a054-438c-80a8-7212ae953cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!uv pip install -q --system tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54ac5ab0-1549-4d2c-9920-9a0c8d28c7d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total markdown files: 43\n"
     ]
    }
   ],
   "source": [
    "md_docs = [mf for mf in DATA_DIR.rglob(\"*.md\") if \"redacted\" not in mf.name]\n",
    "print(f\"Total markdown files: {len(md_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01f19467-af55-4e98-a1b6-f950a1c41160",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-02 AAA Clause Pg.5 Sect. 19-redacted.md ğŸ‘‰ tokens: 5364\n",
      "2021-07-08 Claimant's Demand for Arbitration-redacted.md ğŸ‘‰ tokens: 736\n",
      "2021-08-02 Respondent's Answering Statement and Counterclaim-redacted.md ğŸ‘‰ tokens: 527\n",
      "2021-08-02 Respondent's Supplemental Attachment to Answering Statement and Counterclaim-redacted.md ğŸ‘‰ tokens: 413\n",
      "2021-08-03 Claimant's Answer to Counterclaim-redacted.md ğŸ‘‰ tokens: 224\n",
      "2021-09-22 Respondent's Counterclaim Withdrawal-redacted.md ğŸ‘‰ tokens: 1094\n",
      "2021-09-24 Claimant's Amended Claim Amount plus Interest and Penalty-redacted.md ğŸ‘‰ tokens: 328\n",
      "2021-09-24 Claimant's Amended Claim Amount plus Interest-redacted.md ğŸ‘‰ tokens: 327\n",
      "2021-12-07 Arbitrator's Final Award-redacted.md ğŸ‘‰ tokens: 1095\n",
      "012100046665_6365_28557680_Claimant Case Stated-redacted.md ğŸ‘‰ tokens: 498\n",
      "012100046665_6366_28557682_C-28 Claimant job outline-redacted.md ğŸ‘‰ tokens: 2170\n",
      "012100046665_6368_28557688_C-26 Claimant bio-redacted.md ğŸ‘‰ tokens: 665\n",
      "012100046665_6369_28557690_C-25a, 25b 4.11.21 invoices-redacted.md ğŸ‘‰ tokens: 472\n",
      "012100046665_6370_28557693_C-24a-m finished job-redacted.md ğŸ‘‰ tokens: 199\n",
      "012100046665_6371_28557694_C-23a, 23b garage door-redacted.md ğŸ‘‰ tokens: 14\n",
      "012100046665_6372_28557695_C-22a, 22b 1.7.21 invoice and partial payment-redacted.md ğŸ‘‰ tokens: 18\n",
      "012100046665_6373_28557696_C-21a, 21b, 21c, 21d, 21e, 21f exterior and loft-redacted.md ğŸ‘‰ tokens: 42\n",
      "012100046665_6375_28557698_C-19a, 19b 12.15.20 invoice and payment-redacted.md ğŸ‘‰ tokens: 565\n",
      "012100046665_6376_28557699_C-18a, 18b 10.29.20 invoice and payment-redacted.md ğŸ‘‰ tokens: 216\n",
      "012100046665_6377_28557700_C-17a, 17b materials moved from tent-redacted.md ğŸ‘‰ tokens: 14\n",
      "012100046665_6378_28557701_C-16a, 16b sides tarped-redacted.md ğŸ‘‰ tokens: 14\n",
      "012100046665_6379_28557702_C-15 10.13.20 invoice-redacted.md ğŸ‘‰ tokens: 3\n",
      "012100046665_6380_28557703_C-14a, 14b building under roof-redacted.md ğŸ‘‰ tokens: 14\n",
      "012100046665_6381_28557704_C-13 inside tent 10.5.2020-redacted.md ğŸ‘‰ tokens: 7\n",
      "012100046665_6382_28557705_C-12 post and beam frame-redacted.md ğŸ‘‰ tokens: 7\n",
      "012100046665_6383_28557706_C-11 post frame timbers-redacted.md ğŸ‘‰ tokens: 7\n",
      "012100046665_6384_28557707_C-10 discolored wood and staining-redacted.md ğŸ‘‰ tokens: 7\n",
      "012100046665_6385_28557708_C-9 tent-redacted.md ğŸ‘‰ tokens: 7\n",
      "012100046665_6387_28557710_C-7a, 7b damaged lumber-redacted.md ğŸ‘‰ tokens: 14\n",
      "012100046665_6388_28557711_C-6a, 6b, 6c Aug invoices and payments-redacted.md ğŸ‘‰ tokens: 157\n",
      "012100046665_6389_28557712_C-5a, 5b, 5c, 5d, 5e, 5f concrete footers-redacted.md ğŸ‘‰ tokens: 42\n",
      "012100046665_6390_28557713_C-4a, 4b, 4c excavation-redacted.md ğŸ‘‰ tokens: 21\n",
      "012100046665_6391_28557714_C-3a, 3b footer-redacted.md ğŸ‘‰ tokens: 13\n",
      "012100046665_6392_28557715_C-2 Initial Deposit-redacted.md ğŸ‘‰ tokens: 3\n",
      "2022-06-26 Respondent's Answering Statement-redacted.md ğŸ‘‰ tokens: 973\n",
      "2022-10-17 Arbitrator's Final Award-redacted.md ğŸ‘‰ tokens: 784\n",
      "2022-03-25 AAA Clause Pg. 14 Sect. 24-redacted.md ğŸ‘‰ tokens: 10329\n",
      "2022-05-11 Claimant's Demand for Arbitration-redacted.md ğŸ‘‰ tokens: 815\n",
      "2023-11-17 Arbitrator's Schedule Order-redacted.md ğŸ‘‰ tokens: 478\n",
      "2024-02-16 Arbitrator's Award-redacted.md ğŸ‘‰ tokens: 1950\n",
      "012300021267_44839_33444916_Exhibit D - Invoice-redacted.md ğŸ‘‰ tokens: 142\n",
      "012300021267_44840_33444918_Exhibit C - Demand Letter-redacted.md ğŸ‘‰ tokens: 851\n",
      "012300021267_44842_33444920_Exhibit A - Contract-redacted.md ğŸ‘‰ tokens: 8127\n",
      "Mean tok/doc: 924.3255813953489\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# This is not LLaMA 3 tokenizer, but will just give as an idea...\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "total_toks = 0\n",
    "for doc in md_docs:\n",
    "    # if \"redacted\" in doc.name:\n",
    "    #     continue\n",
    "        \n",
    "    text = doc.open().read()\n",
    "    tokens = enc.encode(text)\n",
    "    total_toks += len(tokens)\n",
    "    print(f\"{doc.name} ğŸ‘‰ tokens: {len(tokens)}\")\n",
    "    \n",
    "print(f\"Mean tok/doc: {total_toks/len(md_docs)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
